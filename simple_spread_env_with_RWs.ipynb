{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7a0af50-8087-412c-9a9f-1dc0f33d3b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_spread_v3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4a4c471-5011-40f9-a3e5-d5beb439b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of agents in the environment\n",
    "num_agents = 3\n",
    "\n",
    "# Initialize the 'simple_spread_v2' environment with specified parameters\n",
    "env = simple_spread_v3.env(\n",
    "    N=num_agents,                 # Number of agents and landmarks in the environment\n",
    "    max_cycles=25,                # Maximum number of steps (cycles) per episode\n",
    "    local_ratio=0.5,              # Weight applied to local versus global rewards\n",
    "    continuous_actions=False,     # Use discrete action space\n",
    "    render_mode='rgb_array',      # Set the rendering mode to return RGB frames\n",
    "    # render_mode = 'human'    # display the environment's state in a window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "480ef51e-db3a-4a0b-8b08-151eaedb74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74c13540-188c-4ab0-ab66-a8136ece2bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent_0', 'agent_1', 'agent_2']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96bfa165-fdf3-4731-9d25-2ba81c6a8cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_0 Box(-inf, inf, (18,), float32)\n",
      "agent_1 Box(-inf, inf, (18,), float32)\n",
      "agent_2 Box(-inf, inf, (18,), float32)\n"
     ]
    }
   ],
   "source": [
    "for agent in env.agents:\n",
    "    print(agent, env.observation_space(agent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0a466-1d00-48d0-a72a-911595b11f32",
   "metadata": {},
   "source": [
    "## Agents' Observation Space\n",
    "In the `simple_spread_v2` environment from PettingZoo's Multi-Agent Particle Environments (MPE), each agent's observation is represented by a one-dimensional NumPy array with 18 elements, corresponding to the `Box(-inf, inf, (18,), float32)` observation space.\n",
    "\n",
    "This observation vector comprises the following components:\n",
    "\n",
    "1. **Agent's Own Velocity (`self_vel`)**:\n",
    "   - **Dimensions**: 2\n",
    "   - **Description**: The agent's current velocity in the 2D plane, represented by its x and y components.\n",
    "\n",
    "2. **Agent's Own Position (`self_pos`)**:\n",
    "   - **Dimensions**: 2\n",
    "   - **Description**: The agent's current position coordinates in the environment.\n",
    "\n",
    "3. **Relative Positions of Landmarks (`landmark_rel_positions`)**:\n",
    "   - **Dimensions**: 2 per landmark\n",
    "   - **Description**: The position of each landmark relative to the agent's current position. With 3 landmarks, this results in 6 dimensions.\n",
    "\n",
    "4. **Relative Positions of Other Agents (`other_agent_rel_positions`)**:\n",
    "   - **Dimensions**: 2 per other agent\n",
    "   - **Description**: The positions of other agents relative to the current agent's position. With 2 other agents, this accounts for 4 dimensions.\n",
    "\n",
    "5. **Communication from Other Agents (`communication`)**:\n",
    "   - **Dimensions**: 2\n",
    "   - **Description**: Communication signals received from other agents. In this environment, agents are silent, so this component is typically zeroed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "595eea06-e656-4d15-a03c-9e02e8d03254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_0 Discrete(5)\n",
      "agent_1 Discrete(5)\n",
      "agent_2 Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "for agent in env.agents:\n",
    "    print(agent, env.action_space(agent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e0e8cf-3421-4fce-a1c5-8d9bd9e7abf0",
   "metadata": {},
   "source": [
    "## Action space\n",
    "This indicates that the agent can choose from 5 distinct actions, typically corresponding to:\n",
    "\n",
    "No Action: The agent remains stationary.\n",
    "Move Left: The agent moves to the left.\n",
    "Move Right: The agent moves to the right.\n",
    "Move Down: The agent moves downward.\n",
    "Move Up: The agent moves upward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6a4d847-632c-4f21-aed0-747ce9dceed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def close(observation, num_agents=3, proximity_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Determines if an agent is within a specified proximity to any other agent.\n",
    "\n",
    "    Parameters:\n",
    "        observation (np.ndarray): The observation vector of the agent.\n",
    "        num_agents (int): Total number of agents in the environment.\n",
    "        proximity_threshold (float): Distance threshold to consider \"close proximity\".\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the agent is close to another agent, False otherwise.\n",
    "    \"\"\"\n",
    "    # Length of each relative position vector (assuming 2D space)\n",
    "    rel_pos_len = 2\n",
    "\n",
    "    # Extract the relative positions of other agents from the observation\n",
    "    start_index = 4 + num_agents * rel_pos_len  # Skip self_vel, self_pos, and landmark_rel_positions\n",
    "    other_agent_rel_positions = observation[start_index:start_index + (num_agents - 1) * rel_pos_len]\n",
    "\n",
    "    # Iterate through each relative position\n",
    "    for i in range(0, len(other_agent_rel_positions), rel_pos_len):\n",
    "        rel_pos = other_agent_rel_positions[i:i + rel_pos_len]\n",
    "        distance = np.linalg.norm(rel_pos)  # Calculate Euclidean distance\n",
    "        print(f\"distance is {distance}\")\n",
    "        if distance < proximity_threshold:\n",
    "            return True\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63bb9b38-ba08-465e-94e3-439b21ac64ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation is [ 0.          0.         -0.94738203 -0.71392    -0.04125339  1.329869\n",
      "  1.697496    0.15351829  0.54906064 -0.20961536  0.13211206 -0.15654828\n",
      "  0.2871902   1.54135     0.          0.          0.          0.        ]\n",
      "distance is 0.20484374463558197\n",
      "distance is 1.567876935005188\n",
      "agent_0 is not in close proximity to another agent.\n",
      "observation is [ 0.          0.         -0.81527    -0.87046826 -0.17336544  1.4864174\n",
      "  1.565384    0.31006655  0.4169486  -0.05306709 -0.13211206  0.15654828\n",
      "  0.15507813  1.6978983   0.          0.          0.          0.        ]\n",
      "distance is 0.20484374463558197\n",
      "distance is 1.704965591430664\n",
      "agent_1 is not in close proximity to another agent.\n",
      "observation is [ 0.          0.         -0.6601919   0.82743    -0.3284436  -0.21148095\n",
      "  1.4103059  -1.3878318   0.26187047 -1.7509654  -0.2871902  -1.54135\n",
      " -0.15507813 -1.6978983   0.          0.          0.          0.        ]\n",
      "distance is 1.567876935005188\n",
      "distance is 1.704965591430664\n",
      "agent_2 is not in close proximity to another agent.\n"
     ]
    }
   ],
   "source": [
    "# test the labelling function close()\n",
    "env.reset()\n",
    "\n",
    "# Number of agents\n",
    "num_agents = len(env.agents)\n",
    "\n",
    "# Iterate through agents to check proximity\n",
    "for agent in env.agents:\n",
    "    observation = env.observe(agent)\n",
    "    print(f\"observation is {observation}\")\n",
    "    if close(observation, num_agents):\n",
    "        print(f\"{agent} is in close proximity to another agent.\")\n",
    "    else:\n",
    "        print(f\"{agent} is not in close proximity to another agent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3ca75aa-666c-4854-94ee-014254fa2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def landmark(observation, arrival_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Determines if the agent has arrived at any landmark.\n",
    "\n",
    "    Parameters:\n",
    "    - observation: numpy array containing the agent's observation.\n",
    "    - arrival_threshold: float, the distance below which a landmark is considered 'arrived at'.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the agent has arrived at any landmark, False otherwise.\n",
    "    \"\"\"\n",
    "    # Extract the number of landmarks from the observation\n",
    "    num_landmarks = 3  # Each landmark has an (x, y) position\n",
    "\n",
    "    # Iterate over each landmark's relative position\n",
    "    for i in range(num_landmarks):\n",
    "        # Extract the relative position (x, y) of the landmark\n",
    "        rel_pos = observation[2 + 2 * i : 4 + 2 * i]\n",
    "        # Calculate the Euclidean distance\n",
    "        distance = np.linalg.norm(rel_pos)\n",
    "        print(f\"distance to landmark{i} is {distance}\")\n",
    "        # Check if the distance is below the arrival threshold\n",
    "        if distance < arrival_threshold:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f19c12e-7873-46bd-8c1c-de175954a629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation is [ 0.          0.          0.54260415 -0.7632381  -0.15707378  0.97000307\n",
      " -0.571121    1.7438174   0.05553359 -0.216192   -1.1693283  -0.07309182\n",
      " -1.1375309   1.5838761   0.          0.          0.          0.        ]\n",
      "distance to landmark0 is 0.9364569187164307\n",
      "distance to landmark1 is 0.9826382994651794\n",
      "distance to landmark2 is 1.8349601030349731\n",
      "agent_0 has not arrived at any landmark.\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'env' is your initialized simple_spread_v2 environment\n",
    "observations = env.reset()\n",
    "agent_id = 'agent_0'  # Example agent ID\n",
    "\n",
    "# Get the observation for the specified agent\n",
    "observation = env.observe(agent)\n",
    "print(f\"observation is {observation}\")\n",
    "\n",
    "# Check if the agent has arrived at any landmark\n",
    "if landmark(observation):\n",
    "    print(f\"{agent_id} has arrived at a landmark.\")\n",
    "else:\n",
    "    print(f\"{agent_id} has not arrived at any landmark.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b6e5ac2-0529-41cf-be20-bacd9b951492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def towards(observation, num_agents=3, num_landmarks=3):\n",
    "    \"\"\"\n",
    "    Determines if the agent is heading towards a landmark that another agent is closer to.\n",
    "\n",
    "    Parameters:\n",
    "    - observation: numpy array containing the agent's observation.\n",
    "    - num_agents: int, total number of agents in the environment.\n",
    "    - num_landmarks: int, total number of landmarks in the environment.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the agent is heading towards a landmark that another agent is closer to, False otherwise.\n",
    "    \"\"\"\n",
    "    # Extract agent's velocity and position\n",
    "    self_vel = observation[0:2]\n",
    "    self_pos = observation[2:4]\n",
    "\n",
    "    # Extract relative positions of landmarks\n",
    "    landmark_rel_positions = observation[4:4 + 2 * num_landmarks].reshape((num_landmarks, 2))\n",
    "\n",
    "    # Extract relative positions of other agents\n",
    "    other_agent_rel_positions = observation[4 + 2 * num_landmarks:-4].reshape((num_agents - 1, 2))\n",
    "\n",
    "    # Calculate absolute positions of landmarks\n",
    "    landmark_positions = self_pos + landmark_rel_positions\n",
    "\n",
    "    # Initialize a flag to indicate if the agent is heading towards an occupied landmark\n",
    "    heading_towards_occupied = False\n",
    "\n",
    "    # Iterate over each landmark\n",
    "    for landmark_pos in landmark_positions:\n",
    "        # Calculate distance from the agent to the landmark\n",
    "        self_to_landmark = np.linalg.norm(landmark_pos - self_pos)\n",
    "\n",
    "        # Assume the current agent is the closest to the landmark\n",
    "        closest_agent_distance = self_to_landmark\n",
    "\n",
    "        # Iterate over each other agent's relative position\n",
    "        for rel_pos in other_agent_rel_positions:\n",
    "            # Calculate the absolute position of the other agent\n",
    "            other_agent_pos = self_pos + rel_pos\n",
    "            # Calculate distance from the other agent to the landmark\n",
    "            other_to_landmark = np.linalg.norm(landmark_pos - other_agent_pos)\n",
    "            # Update the closest agent distance if the other agent is closer\n",
    "            if other_to_landmark < closest_agent_distance:\n",
    "                closest_agent_distance = other_to_landmark\n",
    "\n",
    "        # Check if the current agent is not the closest to the landmark\n",
    "        if closest_agent_distance < self_to_landmark:\n",
    "            # Calculate the vector from the agent to the landmark\n",
    "            to_landmark_vector = landmark_pos - self_pos\n",
    "            # Normalize the vectors\n",
    "            if np.linalg.norm(self_vel) > 0:\n",
    "                self_vel_normalized = self_vel / np.linalg.norm(self_vel)\n",
    "            else:\n",
    "                self_vel_normalized = self_vel\n",
    "            if np.linalg.norm(to_landmark_vector) > 0:\n",
    "                to_landmark_normalized = to_landmark_vector / np.linalg.norm(to_landmark_vector)\n",
    "            else:\n",
    "                to_landmark_normalized = to_landmark_vector\n",
    "            # Calculate the dot product to determine if the agent is heading towards the landmark\n",
    "            dot_product = np.dot(self_vel_normalized, to_landmark_normalized)\n",
    "            print(f\"two unit vectors:{self_vel_normalized, to_landmark_normalized}\")\n",
    "            print(f\"dot_product: {dot_product}\")\n",
    "            if dot_product > 0.87:  # Adjust the threshold as needed (cos(30) ~= 0.87)\n",
    "                heading_towards_occupied = True\n",
    "                break\n",
    "\n",
    "    return heading_towards_occupied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81e824ad-d9a8-448f-b017-635f1c463b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation is [ 0.          0.          0.510663   -0.1971223  -1.0898983   0.9509414\n",
      " -1.0775567   0.92513096 -0.37778127 -0.58092666 -1.1393815   0.9458253\n",
      " -1.2440398   0.11394595  0.          0.          0.          0.        ]\n",
      "two unit vectors:(array([0., 0.], dtype=float32), array([-0.75350773,  0.65743905], dtype=float32))\n",
      "dot_product: 0.0\n",
      "two unit vectors:(array([0., 0.], dtype=float32), array([-0.7587307 ,  0.65140444], dtype=float32))\n",
      "dot_product: 0.0\n",
      "agent_0 is not heading towards a landmark occupied by another agent.\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'env' is your initialized simple_spread_v2 environment\n",
    "observations = env.reset()\n",
    "agent_id = 'agent_0'  # Example agent ID\n",
    "agent_index = 0  # Index corresponding to agent_0\n",
    "\n",
    "# Get the observation for the specified agent\n",
    "observation = env.observe(agent)\n",
    "print(f\"observation is {observation}\")\n",
    "\n",
    "# Check if the agent is heading towards a landmark that another agent is closer to\n",
    "if towards(observation, agent_index):\n",
    "    print(f\"{agent_id} is heading towards a landmark occupied by another agent.\")\n",
    "else:\n",
    "    print(f\"{agent_id} is not heading towards a landmark occupied by another agent.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dde1e886-c09d-4206-8482-8bf31b2f3792",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RewardMachine:\n",
    "    def __init__(self):\n",
    "        self.state = 'u0'  # Initial state\n",
    "\n",
    "    def transition(self, event):\n",
    "        \"\"\"\n",
    "        Transitions the RM state based on the given event and returns the associated reward.\n",
    "        u0: the initial state\n",
    "        u1: the agent is moving to a not good landmark\n",
    "        u2: the agent has arrived a landmark\n",
    "        u3: the agent is very close to another agent\n",
    "        \"\"\"\n",
    "        if self.state == 'u0':\n",
    "            if event == 'towards':\n",
    "                self.state = 'u1'\n",
    "                return -1\n",
    "            elif event == 'landmark':\n",
    "                self.state = 'u2'\n",
    "                return 10\n",
    "            elif event == 'close':\n",
    "                self.state = 'u3'\n",
    "                return -10\n",
    "            else:\n",
    "                return 0\n",
    "        elif self.state == 'u1':\n",
    "            if event == 'towards':\n",
    "                return -1\n",
    "            elif event == 'landmark':\n",
    "                self.state = 'u2'\n",
    "                return 10\n",
    "            elif event == 'close':\n",
    "                self.state = 'u3'\n",
    "                return -10\n",
    "            else:\n",
    "                self.state = 'u0'\n",
    "                return 0\n",
    "        elif self.state == 'u2':\n",
    "            return 0\n",
    "        elif self.state == 'u3':\n",
    "            if event == 'close':\n",
    "                return -10\n",
    "            elif event == 'towards':\n",
    "                self.state = 'u1'\n",
    "                return 0\n",
    "            else:\n",
    "                self.state = 'u0'\n",
    "                return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37c4e2fb-3e9c-4482-bdbc-6cfaecde93ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation is [ 0.          0.         -0.821315    0.81473726  0.5756123  -1.3639863\n",
      " -0.09783769 -1.4809564   0.07975288 -0.88076395  0.52989763 -0.4707043\n",
      "  0.5894765  -0.5078848   0.          0.          0.          0.        ]\n",
      "distance to landmark0 is 1.1568729877471924\n",
      "distance to landmark1 is 1.4804688692092896\n",
      "distance to landmark2 is 1.484184741973877\n",
      "distance is 0.7087693810462952\n",
      "distance is 0.7780935168266296\n",
      "two unit vectors:(array([0., 0.], dtype=float32), array([ 0.38880405, -0.92132044], dtype=float32))\n",
      "dot_product: 0.0\n",
      "two unit vectors:(array([0., 0.], dtype=float32), array([-0.06592015, -0.99782485], dtype=float32))\n",
      "dot_product: 0.0\n",
      "two unit vectors:(array([0., 0.], dtype=float32), array([ 0.09018069, -0.99592537], dtype=float32))\n",
      "dot_product: 0.0\n",
      "the event: none\n",
      "the RM state: u0\n",
      "the reward: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming 'env' is your initialized simple_spread_v2 environment\n",
    "observations = env.reset()\n",
    "agent_id = 'agent_0'  # Example agent ID\n",
    "agent_index = 0  # Index corresponding to agent_0\n",
    "\n",
    "# Get the observation for the specified agent\n",
    "agent_obs = env.observe(agent)\n",
    "print(f\"observation is {agent_obs}\")\n",
    "\n",
    "# Example integration with the environment\n",
    "def compute_reward(agent_obs, reward_machine):\n",
    "    \"\"\"\n",
    "    Computes the reward for the agent based on its observation and the reward machine.\n",
    "    \"\"\"\n",
    "    if landmark(agent_obs):\n",
    "        event = 'landmark'\n",
    "    elif close(agent_obs):\n",
    "        event = 'close'\n",
    "    elif towards(agent_obs):\n",
    "        event = 'towards'\n",
    "    else:\n",
    "        event = 'none'\n",
    "\n",
    "    reward = reward_machine.transition(event)\n",
    "    print(f\"the event: {event}\")\n",
    "    print(f\"the RM state: {reward_machine.state}\")\n",
    "    print(f\"the reward: {reward}\")\n",
    "    return reward\n",
    "\n",
    "reward_machine = RewardMachine()\n",
    "\n",
    "compute_reward(agent_obs, reward_machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eae8b8d7-ae45-4f97-9fd0-88906814a97c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance to landmark0 is 1.152658462524414\n",
      "distance to landmark1 is 1.0067602396011353\n",
      "distance to landmark2 is 1.304940104484558\n",
      "distance is 0.9787635803222656\n",
      "distance is 1.0965368747711182\n",
      "two unit vectors:(array([0., 0.], dtype=float32), array([0.6514576 , 0.75868505], dtype=float32))\n",
      "dot_product: 0.0\n",
      "two unit vectors:(array([0., 0.], dtype=float32), array([ 0.9990368 , -0.04388209], dtype=float32))\n",
      "dot_product: 0.0\n",
      "two unit vectors:(array([0., 0.], dtype=float32), array([0.8736684 , 0.48652202], dtype=float32))\n",
      "dot_product: 0.0\n",
      "the event: none\n",
      "the RM state: u0\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.9704020619392395\n",
      "distance to landmark1 is 0.8317274451255798\n",
      "distance to landmark2 is 0.32947659492492676\n",
      "distance is 0.9787635803222656\n",
      "distance is 0.9243974089622498\n",
      "two unit vectors:(array([0., 0.], dtype=float32), array([-0.38822612,  0.92156416], dtype=float32))\n",
      "dot_product: 0.0\n",
      "two unit vectors:(array([0., 0.], dtype=float32), array([0.4780336 , 0.87834156], dtype=float32))\n",
      "dot_product: 0.0\n",
      "the event: none\n",
      "the RM state: u0\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.058738209307193756\n",
      "the event: landmark\n",
      "the RM state: u2\n",
      "the reward: 10\n",
      "distance to landmark0 is 1.152658462524414\n",
      "distance to landmark1 is 1.0067602396011353\n",
      "distance to landmark2 is 1.304940104484558\n",
      "distance is 0.9787635803222656\n",
      "distance is 1.0965368747711182\n",
      "two unit vectors:(array([-0.,  1.], dtype=float32), array([0.6514576 , 0.75868505], dtype=float32))\n",
      "dot_product: 0.7586850523948669\n",
      "two unit vectors:(array([-0.,  1.], dtype=float32), array([ 0.9990368 , -0.04388209], dtype=float32))\n",
      "dot_product: -0.043882086873054504\n",
      "two unit vectors:(array([-0.,  1.], dtype=float32), array([0.8736684 , 0.48652202], dtype=float32))\n",
      "dot_product: 0.48652201890945435\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.9704020619392395\n",
      "distance to landmark1 is 0.8317274451255798\n",
      "distance to landmark2 is 0.32947659492492676\n",
      "distance is 0.9787635803222656\n",
      "distance is 0.9243974089622498\n",
      "two unit vectors:(array([-1., -0.], dtype=float32), array([-0.38822612,  0.92156416], dtype=float32))\n",
      "dot_product: 0.38822612166404724\n",
      "two unit vectors:(array([-1., -0.], dtype=float32), array([0.4780336 , 0.87834156], dtype=float32))\n",
      "dot_product: -0.4780336022377014\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.058738209307193756\n",
      "the event: landmark\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.1129951477050781\n",
      "distance to landmark1 is 0.9693734645843506\n",
      "distance to landmark2 is 1.3080884218215942\n",
      "distance is 0.9302525520324707\n",
      "distance is 1.026450753211975\n",
      "two unit vectors:(array([-0.,  1.], dtype=float32), array([0.676583  , 0.73636633], dtype=float32))\n",
      "dot_product: 0.736366331577301\n",
      "two unit vectors:(array([-0.,  1.], dtype=float32), array([ 0.9966323 , -0.08200015], dtype=float32))\n",
      "dot_product: -0.08200015127658844\n",
      "two unit vectors:(array([-0.,  1.], dtype=float32), array([0.88673604, 0.46227604], dtype=float32))\n",
      "dot_product: 0.46227604150772095\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.9566932916641235\n",
      "distance to landmark1 is 0.813621997833252\n",
      "distance to landmark2 is 0.3788761496543884\n",
      "distance is 0.9302525520324707\n",
      "distance is 0.9243974089622498\n",
      "two unit vectors:(array([-1., -0.], dtype=float32), array([-0.33541167,  0.9420717 ], dtype=float32))\n",
      "dot_product: 0.3354116678237915\n",
      "two unit vectors:(array([-1., -0.], dtype=float32), array([0.51942956, 0.8545132 ], dtype=float32))\n",
      "dot_product: -0.5194295644760132\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.08922220766544342\n",
      "the event: landmark\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.0457189083099365\n",
      "distance to landmark1 is 0.9068757891654968\n",
      "distance to landmark2 is 1.3181512355804443\n",
      "distance is 0.8528585433959961\n",
      "distance is 0.9040340781211853\n",
      "two unit vectors:(array([-0.,  1.], dtype=float32), array([0.72321  , 0.6906282], dtype=float32))\n",
      "dot_product: 0.690628170967102\n",
      "two unit vectors:(array([-0.,  1.], dtype=float32), array([ 0.989024  , -0.14775504], dtype=float32))\n",
      "dot_product: -0.14775504171848297\n",
      "two unit vectors:(array([-0.,  1.], dtype=float32), array([0.9087968 , 0.41723904], dtype=float32))\n",
      "dot_product: 0.4172390401363373\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.9386487603187561\n",
      "distance to landmark1 is 0.7885935306549072\n",
      "distance to landmark2 is 0.46563389897346497\n",
      "distance is 0.8528585433959961\n",
      "distance is 0.9243974089622498\n",
      "two unit vectors:(array([-0.79543173, -0.6060432 ], dtype=float32), array([-0.23509996,  0.9719712 ], dtype=float32))\n",
      "dot_product: -0.40205058455467224\n",
      "two unit vectors:(array([-0.79543173, -0.6060432 ], dtype=float32), array([0.58387786, 0.8118415 ], dtype=float32))\n",
      "dot_product: -0.9564460515975952\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.1669883131980896\n",
      "distance to landmark1 is 0.16215988993644714\n",
      "distance to landmark2 is 1.2051852941513062\n",
      "distance is 0.9040340781211853\n",
      "distance is 0.9243974089622498\n",
      "two unit vectors:(array([-1.,  0.], dtype=float32), array([ 0.64032394, -0.768105  ], dtype=float32))\n",
      "dot_product: -0.6403239369392395\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.9973235726356506\n",
      "distance to landmark1 is 0.8628595471382141\n",
      "distance to landmark2 is 1.3294329643249512\n",
      "distance is 0.8167275190353394\n",
      "distance is 0.8124855160713196\n",
      "two unit vectors:(array([0.71265244, 0.7015173 ], dtype=float32), array([0.7601024, 0.6498033], dtype=float32))\n",
      "dot_product: 0.9975370168685913\n",
      "the event: towards\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.9800975322723389\n",
      "distance to landmark1 is 0.8252284526824951\n",
      "distance to landmark2 is 0.5280680656433105\n",
      "distance is 0.8167275190353394\n",
      "distance is 0.971659779548645\n",
      "two unit vectors:(array([-0.93541795, -0.3535438 ], dtype=float32), array([-0.14513959,  0.9894112 ], dtype=float32))\n",
      "dot_product: -0.21403399109840393\n",
      "two unit vectors:(array([-0.93541795, -0.3535438 ], dtype=float32), array([0.60239315, 0.7981996 ], dtype=float32))\n",
      "dot_product: -0.8456878662109375\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.22995063662528992\n",
      "distance to landmark1 is 0.21647794544696808\n",
      "distance to landmark2 is 1.2482247352600098\n",
      "distance is 0.8124855160713196\n",
      "distance is 0.971659779548645\n",
      "two unit vectors:(array([-0.7015173 ,  0.71265244], dtype=float32), array([ 0.6708198, -0.7416203], dtype=float32))\n",
      "dot_product: -0.9991092681884766\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.9271966814994812\n",
      "distance to landmark1 is 0.7928872108459473\n",
      "distance to landmark2 is 1.2913473844528198\n",
      "distance is 0.7139448523521423\n",
      "distance is 0.7605761289596558\n",
      "two unit vectors:(array([0.71265244, 0.7015172 ], dtype=float32), array([0.7641208 , 0.64507306], dtype=float32))\n",
      "dot_product: 0.9970824122428894\n",
      "the event: towards\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.0138975381851196\n",
      "distance to landmark1 is 0.8542374968528748\n",
      "distance to landmark2 is 0.628129780292511\n",
      "distance is 0.7139448523521423\n",
      "distance is 1.0414156913757324\n",
      "two unit vectors:(array([-0.93541795, -0.3535438 ], dtype=float32), array([-0.02406188,  0.9997105 ], dtype=float32))\n",
      "dot_product: -0.330933541059494\n",
      "two unit vectors:(array([-0.93541795, -0.3535438 ], dtype=float32), array([0.64133966, 0.7672571 ], dtype=float32))\n",
      "dot_product: -0.8711795806884766\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.2724974453449249\n",
      "distance to landmark1 is 0.28443533182144165\n",
      "distance to landmark2 is 1.3183259963989258\n",
      "distance is 0.7605761289596558\n",
      "distance is 1.0414156913757324\n",
      "two unit vectors:(array([-0.9181818 ,  0.39615935], dtype=float32), array([ 0.6724837 , -0.74011195], dtype=float32))\n",
      "dot_product: -0.9106645584106445\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.8746050000190735\n",
      "distance to landmark1 is 0.7404313087463379\n",
      "distance to landmark2 is 1.2645862102508545\n",
      "distance is 0.6563664078712463\n",
      "distance is 0.7094952464103699\n",
      "two unit vectors:(array([0.71265244, 0.7015173 ], dtype=float32), array([0.7676089 , 0.64091855], dtype=float32))\n",
      "dot_product: 0.9966537952423096\n",
      "the event: towards\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.045611023902893\n",
      "distance to landmark1 is 0.8837578892707825\n",
      "distance to landmark2 is 0.7043307423591614\n",
      "distance is 0.6563664078712463\n",
      "distance is 1.1080944538116455\n",
      "two unit vectors:(array([-0.26557142, -0.9640912 ], dtype=float32), array([0.06094372, 0.9981412 ], dtype=float32))\n",
      "dot_product: -0.9784840941429138\n",
      "two unit vectors:(array([-0.26557142, -0.9640912 ], dtype=float32), array([0.6658055 , 0.74612534], dtype=float32))\n",
      "dot_product: -0.8961517810821533\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.3608115613460541\n",
      "distance to landmark1 is 0.37809959053993225\n",
      "distance to landmark2 is 1.4050729274749756\n",
      "distance is 0.7094952464103699\n",
      "distance is 1.1080944538116455\n",
      "two unit vectors:(array([-0.47510162,  0.879931  ], dtype=float32), array([ 0.69282293, -0.72110766], dtype=float32))\n",
      "dot_product: -0.9636862277984619\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.8351637125015259\n",
      "distance to landmark1 is 0.701105535030365\n",
      "distance to landmark2 is 1.2455973625183105\n",
      "distance is 0.6627132892608643\n",
      "distance is 0.6962909698486328\n",
      "two unit vectors:(array([-0.8121779 ,  0.58340985], dtype=float32), array([0.7705497 , 0.63737994], dtype=float32))\n",
      "dot_product: -0.2539696991443634\n",
      "two unit vectors:(array([-0.8121779 ,  0.58340985], dtype=float32), array([ 0.9538059, -0.3004236], dtype=float32))\n",
      "dot_product: -0.9499301910400391\n",
      "two unit vectors:(array([-0.8121779 ,  0.58340985], dtype=float32), array([0.94051975, 0.33973888], dtype=float32))\n",
      "dot_product: -0.5656623244285583\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.0671249628067017\n",
      "distance to landmark1 is 0.905177891254425\n",
      "distance to landmark2 is 0.712242841720581\n",
      "distance is 0.6627132892608643\n",
      "distance is 1.1581130027770996\n",
      "two unit vectors:(array([ 0.9448508 , -0.32750112], dtype=float32), array([0.06592079, 0.99782485], dtype=float32))\n",
      "dot_product: -0.26450344920158386\n",
      "two unit vectors:(array([ 0.9448508 , -0.32750112], dtype=float32), array([0.6598853, 0.7513663], dtype=float32))\n",
      "dot_product: 0.3774198293685913\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.3793919086456299\n",
      "distance to landmark1 is 0.4058060348033905\n",
      "distance to landmark2 is 1.4359002113342285\n",
      "distance is 0.6962909698486328\n",
      "distance is 1.1581130027770996\n",
      "two unit vectors:(array([-0.9457282,  0.3249589], dtype=float32), array([ 0.68852437, -0.7252132 ], dtype=float32))\n",
      "dot_product: -0.8868213891983032\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.8406853079795837\n",
      "distance to landmark1 is 0.7109783887863159\n",
      "distance to landmark2 is 1.2794547080993652\n",
      "distance is 0.742378294467926\n",
      "distance is 0.6880127191543579\n",
      "two unit vectors:(array([-0.977203,  0.212307], dtype=float32), array([0.8005066 , 0.59932405], dtype=float32))\n",
      "dot_product: -0.6550167798995972\n",
      "two unit vectors:(array([-0.977203,  0.212307], dtype=float32), array([ 0.9511586 , -0.30870256], dtype=float32))\n",
      "dot_product: -0.9950147271156311\n",
      "two unit vectors:(array([-0.977203,  0.212307], dtype=float32), array([0.9474694 , 0.31984648], dtype=float32))\n",
      "dot_product: -0.857964277267456\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.0799071788787842\n",
      "distance to landmark1 is 0.9191363453865051\n",
      "distance to landmark2 is 0.6690584421157837\n",
      "distance is 0.742378294467926\n",
      "distance is 1.22359037399292\n",
      "two unit vectors:(array([ 0.4841507, -0.8749847], dtype=float32), array([0.0152621 , 0.99988353], dtype=float32))\n",
      "dot_product: -0.8674936294555664\n",
      "two unit vectors:(array([ 0.4841507, -0.8749847], dtype=float32), array([0.6316689, 0.7752383], dtype=float32))\n",
      "dot_product: -0.37249863147735596\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.44349488615989685\n",
      "distance to landmark1 is 0.46907439827919006\n",
      "distance to landmark2 is 1.4937667846679688\n",
      "distance is 0.6880127191543579\n",
      "distance is 1.22359037399292\n",
      "two unit vectors:(array([-0.9457282,  0.3249589], dtype=float32), array([ 0.7029487 , -0.71124053], dtype=float32))\n",
      "dot_product: -0.8959223031997681\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.8833507299423218\n",
      "distance to landmark1 is 0.7610461115837097\n",
      "distance to landmark2 is 1.3524607419967651\n",
      "distance is 0.8730792999267578\n",
      "distance is 0.694915771484375\n",
      "two unit vectors:(array([-0.8143039 , -0.58043885], dtype=float32), array([0.8420286 , 0.53943294], dtype=float32))\n",
      "dot_product: -0.9987750053405762\n",
      "two unit vectors:(array([-0.8143039 , -0.58043885], dtype=float32), array([ 0.9528144 , -0.30355343], dtype=float32))\n",
      "dot_product: -0.5996862649917603\n",
      "two unit vectors:(array([-0.8143039 , -0.58043885], dtype=float32), array([0.9554278, 0.2952248], dtype=float32))\n",
      "dot_product: -0.9493685364723206\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.1407965421676636\n",
      "distance to landmark1 is 0.9811025261878967\n",
      "distance to landmark2 is 0.6476462483406067\n",
      "distance is 0.8730792999267578\n",
      "distance is 1.3237314224243164\n",
      "two unit vectors:(array([ 0.4841507, -0.8749847], dtype=float32), array([-0.02059276,  0.999788  ], dtype=float32))\n",
      "dot_product: -0.8847692012786865\n",
      "two unit vectors:(array([ 0.4841507, -0.8749847], dtype=float32), array([0.5897132, 0.8076127], dtype=float32))\n",
      "dot_product: -0.4211387038230896\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.49171146750450134\n",
      "distance to landmark1 is 0.5167933106422424\n",
      "distance to landmark2 is 1.5375360250473022\n",
      "distance is 0.694915771484375\n",
      "distance is 1.3237314224243164\n",
      "two unit vectors:(array([-0.48738736,  0.87318593], dtype=float32), array([ 0.712883  , -0.70128304], dtype=float32))\n",
      "dot_product: -0.9598006010055542\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.9491603374481201\n",
      "distance to landmark1 is 0.8269909620285034\n",
      "distance to landmark2 is 1.393053650856018\n",
      "distance is 0.939613401889801\n",
      "distance is 0.7965603470802307\n",
      "two unit vectors:(array([ 0.31919795, -0.9476881 ], dtype=float32), array([0.8398911, 0.542755 ], dtype=float32))\n",
      "dot_product: -0.24627095460891724\n",
      "two unit vectors:(array([ 0.31919795, -0.9476881 ], dtype=float32), array([ 0.9636411, -0.2672   ], dtype=float32))\n",
      "dot_product: 0.5608144998550415\n",
      "two unit vectors:(array([ 0.31919795, -0.9476881 ], dtype=float32), array([0.95130813, 0.3082415 ], dtype=float32))\n",
      "dot_product: 0.011538803577423096\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.1871775388717651\n",
      "distance to landmark1 is 1.0283173322677612\n",
      "distance to landmark2 is 0.6362875699996948\n",
      "distance is 0.939613401889801\n",
      "distance is 1.4464075565338135\n",
      "two unit vectors:(array([ 0.22143206, -0.9751758 ], dtype=float32), array([-0.04461394,  0.9990043 ], dtype=float32))\n",
      "dot_product: -0.9840837717056274\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.540492057800293\n",
      "distance to landmark1 is 0.5786821842193604\n",
      "distance to landmark2 is 1.605661153793335\n",
      "distance is 0.7965603470802307\n",
      "distance is 1.4464075565338135\n",
      "two unit vectors:(array([-0.8531994 ,  0.52158487], dtype=float32), array([ 0.7041428 , -0.71005833], dtype=float32))\n",
      "dot_product: -0.9711298942565918\n",
      "two unit vectors:(array([-0.8531994 ,  0.52158487], dtype=float32), array([ 0.97174114, -0.23604904], dtype=float32))\n",
      "dot_product: -0.9522085785865784\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.9605788588523865\n",
      "distance to landmark1 is 0.8349769711494446\n",
      "distance to landmark2 is 1.3762751817703247\n",
      "distance is 0.9824159145355225\n",
      "distance is 0.852423369884491\n",
      "two unit vectors:(array([ 0.10094387, -0.9948921 ], dtype=float32), array([0.8202647, 0.5719842], dtype=float32))\n",
      "dot_product: -0.48626184463500977\n",
      "two unit vectors:(array([ 0.10094387, -0.9948921 ], dtype=float32), array([ 0.9683555 , -0.24957494], dtype=float32))\n",
      "dot_product: 0.3460496962070465\n",
      "two unit vectors:(array([ 0.10094387, -0.9948921 ], dtype=float32), array([0.94496554, 0.3271698 ], dtype=float32))\n",
      "dot_product: -0.23011019825935364\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.272302508354187\n",
      "distance to landmark1 is 1.113998293876648\n",
      "distance to landmark2 is 0.6513188481330872\n",
      "distance is 0.9824159145355225\n",
      "distance is 1.6038216352462769\n",
      "two unit vectors:(array([ 0.7280125, -0.6855638], dtype=float32), array([-0.05846737,  0.99828935], dtype=float32))\n",
      "dot_product: -0.7269560098648071\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.6270761489868164\n",
      "distance to landmark1 is 0.66759192943573\n",
      "distance to landmark2 is 1.692183494567871\n",
      "distance is 0.852423369884491\n",
      "distance is 1.6038216352462769\n",
      "two unit vectors:(array([-0.55734855,  0.83027864], dtype=float32), array([ 0.712992 , -0.7011722], dtype=float32))\n",
      "dot_product: -0.9795533418655396\n",
      "two unit vectors:(array([-0.55734855,  0.83027864], dtype=float32), array([ 0.96680945, -0.2554985 ], dtype=float32))\n",
      "dot_product: -0.750984787940979\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.0043259859085083\n",
      "distance to landmark1 is 0.8722172975540161\n",
      "distance to landmark2 is 1.3530701398849487\n",
      "distance is 0.9517238736152649\n",
      "distance is 1.0013054609298706\n",
      "two unit vectors:(array([ 0.10094387, -0.9948921 ], dtype=float32), array([0.7769188 , 0.62960076], dtype=float32))\n",
      "dot_product: -0.5479596853256226\n",
      "two unit vectors:(array([ 0.10094387, -0.9948921 ], dtype=float32), array([ 0.97959703, -0.20097165], dtype=float32))\n",
      "dot_product: 0.29882943630218506\n",
      "two unit vectors:(array([ 0.10094387, -0.9948921 ], dtype=float32), array([0.92908955, 0.36985487], dtype=float32))\n",
      "dot_product: -0.2741798162460327\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.2863316535949707\n",
      "distance to landmark1 is 1.128501057624817\n",
      "distance to landmark2 is 0.644744336605072\n",
      "distance is 0.9517238736152649\n",
      "distance is 1.7224490642547607\n",
      "two unit vectors:(array([ 0.986233  , -0.16536167], dtype=float32), array([-0.070513 ,  0.9975108], dtype=float32))\n",
      "dot_product: -0.23449230194091797\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.7113169431686401\n",
      "distance to landmark1 is 0.762688159942627\n",
      "distance to landmark2 is 1.792346477508545\n",
      "distance is 1.0013054609298706\n",
      "distance is 1.7224490642547607\n",
      "two unit vectors:(array([-0.8245689,  0.5657614], dtype=float32), array([ 0.70490676, -0.7093    ], dtype=float32))\n",
      "dot_product: -0.9825387597084045\n",
      "two unit vectors:(array([-0.8245689,  0.5657614], dtype=float32), array([ 0.95370936, -0.30072987], dtype=float32))\n",
      "dot_product: -0.9565404653549194\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.039196491241455\n",
      "distance to landmark1 is 0.9029034376144409\n",
      "distance to landmark2 is 1.3379414081573486\n",
      "distance is 0.9718501567840576\n",
      "distance is 1.1172319650650024\n",
      "two unit vectors:(array([ 0.045204 , -0.9989778], dtype=float32), array([0.7444837, 0.6676406], dtype=float32))\n",
      "dot_product: -0.6333045363426208\n",
      "two unit vectors:(array([ 0.045204 , -0.9989778], dtype=float32), array([ 0.98660403, -0.16313303], dtype=float32))\n",
      "dot_product: 0.20756471157073975\n",
      "two unit vectors:(array([ 0.045204 , -0.9989778], dtype=float32), array([0.9162163 , 0.40068406], dtype=float32))\n",
      "dot_product: -0.3588578402996063\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.3000984191894531\n",
      "distance to landmark1 is 1.1445362567901611\n",
      "distance to landmark2 is 0.5971426367759705\n",
      "distance is 0.9718501567840576\n",
      "distance is 1.855237603187561\n",
      "two unit vectors:(array([0.73290694, 0.6803289 ], dtype=float32), array([-0.1226742 ,  0.99244696], dtype=float32))\n",
      "dot_product: 0.5852815508842468\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.822147011756897\n",
      "distance to landmark1 is 0.875057578086853\n",
      "distance to landmark2 is 1.9029122591018677\n",
      "distance is 1.1172319650650024\n",
      "distance is 1.855237603187561\n",
      "two unit vectors:(array([-0.92876667,  0.37066483], dtype=float32), array([ 0.7126604, -0.7015093], dtype=float32))\n",
      "dot_product: -0.9219200611114502\n",
      "two unit vectors:(array([-0.92876667,  0.37066483], dtype=float32), array([ 0.947094 , -0.3209563], dtype=float32))\n",
      "dot_product: -0.9985966086387634\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.104527473449707\n",
      "distance to landmark1 is 0.9626597762107849\n",
      "distance to landmark2 is 1.3221462965011597\n",
      "distance is 0.9433658123016357\n",
      "distance is 1.2647229433059692\n",
      "two unit vectors:(array([ 0.02601667, -0.9996615 ], dtype=float32), array([ 0.9953018 , -0.09682185], dtype=float32))\n",
      "dot_product: 0.12268351018428802\n",
      "two unit vectors:(array([ 0.02601667, -0.9996615 ], dtype=float32), array([0.8936408 , 0.44878298], dtype=float32))\n",
      "dot_product: -0.42538151144981384\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.26258385181427\n",
      "distance to landmark1 is 1.109251618385315\n",
      "distance to landmark2 is 0.5361803770065308\n",
      "distance is 0.9433658123016357\n",
      "distance is 1.9409213066101074\n",
      "two unit vectors:(array([ 0.88248223, -0.47034565], dtype=float32), array([-0.1677062,  0.985837 ], dtype=float32))\n",
      "dot_product: -0.6116818785667419\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 0.9505953788757324\n",
      "distance to landmark1 is 1.0003093481063843\n",
      "distance to landmark2 is 2.022165536880493\n",
      "distance is 1.2647229433059692\n",
      "distance is 1.9409213066101074\n",
      "two unit vectors:(array([-0.72251195,  0.69135845], dtype=float32), array([ 0.82757235, -0.5613591 ], dtype=float32))\n",
      "dot_product: -0.9860312938690186\n",
      "two unit vectors:(array([-0.72251195,  0.69135845], dtype=float32), array([ 0.7297374, -0.6837275], dtype=float32))\n",
      "dot_product: -0.9999447464942932\n",
      "two unit vectors:(array([-0.72251195,  0.69135845], dtype=float32), array([ 0.9457754 , -0.32482117], dtype=float32))\n",
      "dot_product: -0.9079018831253052\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.1959469318389893\n",
      "distance to landmark1 is 1.0486232042312622\n",
      "distance to landmark2 is 1.3129123449325562\n",
      "distance is 0.9350754618644714\n",
      "distance is 1.4797472953796387\n",
      "two unit vectors:(array([ 0.01661177, -0.99986196], dtype=float32), array([ 0.99996907, -0.00786448], dtype=float32))\n",
      "dot_product: 0.024474646896123886\n",
      "two unit vectors:(array([ 0.01661177, -0.99986196], dtype=float32), array([0.86261994, 0.5058525 ], dtype=float32))\n",
      "dot_product: -0.49145302176475525\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.2852083444595337\n",
      "distance to landmark1 is 1.1333842277526855\n",
      "distance to landmark2 is 0.5170377492904663\n",
      "distance is 0.9350754618644714\n",
      "distance is 2.094940662384033\n",
      "two unit vectors:(array([-0.87175244, -0.48994648], dtype=float32), array([-0.19432586,  0.980937  ], dtype=float32))\n",
      "dot_product: -0.3112025856971741\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.0688287019729614\n",
      "distance to landmark1 is 1.122833251953125\n",
      "distance to landmark2 is 2.146225929260254\n",
      "distance is 1.4797472953796387\n",
      "distance is 2.094940662384033\n",
      "two unit vectors:(array([-0.97802657,  0.20848028], dtype=float32), array([ 0.81710094, -0.5764947 ], dtype=float32))\n",
      "dot_product: -0.9193341732025146\n",
      "two unit vectors:(array([-0.97802657,  0.20848028], dtype=float32), array([ 0.7293219 , -0.68417066], dtype=float32))\n",
      "dot_product: -0.8559322357177734\n",
      "two unit vectors:(array([-0.97802657,  0.20848028], dtype=float32), array([ 0.9358949 , -0.35227934], dtype=float32))\n",
      "dot_product: -0.9887734055519104\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.3094255924224854\n",
      "distance to landmark1 is 1.1575300693511963\n",
      "distance to landmark2 is 1.3168045282363892\n",
      "distance is 0.8766021132469177\n",
      "distance is 1.6425166130065918\n",
      "two unit vectors:(array([ 0.01661177, -0.9998621 ], dtype=float32), array([0.572558 , 0.8198643], dtype=float32))\n",
      "dot_product: -0.8102400302886963\n",
      "two unit vectors:(array([ 0.01661177, -0.9998621 ], dtype=float32), array([0.9952689 , 0.09715949], dtype=float32))\n",
      "dot_product: -0.080612912774086\n",
      "two unit vectors:(array([ 0.01661177, -0.9998621 ], dtype=float32), array([0.82492924, 0.56523603], dtype=float32))\n",
      "dot_product: -0.5514545440673828\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.2954715490341187\n",
      "distance to landmark1 is 1.1423802375793457\n",
      "distance to landmark2 is 0.5448591113090515\n",
      "distance is 0.8766021132469177\n",
      "distance is 2.1418545246124268\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.1353362798690796\n",
      "distance to landmark1 is 1.1863369941711426\n",
      "distance to landmark2 is 2.2053494453430176\n",
      "distance is 1.6425166130065918\n",
      "distance is 2.1418545246124268\n",
      "two unit vectors:(array([-0.6386794,  0.7694729], dtype=float32), array([ 0.8300322 , -0.55771536], dtype=float32))\n",
      "dot_product: -0.9592713117599487\n",
      "two unit vectors:(array([-0.6386794,  0.7694729], dtype=float32), array([ 0.74025434, -0.6723269 ], dtype=float32))\n",
      "dot_product: -0.9901224970817566\n",
      "two unit vectors:(array([-0.6386794,  0.7694729], dtype=float32), array([ 0.937836 , -0.3470787], dtype=float32))\n",
      "dot_product: -0.8660441637039185\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.3974703550338745\n",
      "distance to landmark1 is 1.2430499792099\n",
      "distance to landmark2 is 1.3291913270950317\n",
      "distance is 0.8491663932800293\n",
      "distance is 1.8140934705734253\n",
      "two unit vectors:(array([-0.53077364, -0.8475136 ], dtype=float32), array([0.5317809, 0.846882 ], dtype=float32))\n",
      "dot_product: -0.9999992847442627\n",
      "two unit vectors:(array([-0.53077364, -0.8475136 ], dtype=float32), array([0.98469776, 0.17427078], dtype=float32))\n",
      "dot_product: -0.6703484654426575\n",
      "two unit vectors:(array([-0.53077364, -0.8475136 ], dtype=float32), array([0.7963583 , 0.60482514], dtype=float32))\n",
      "dot_product: -0.9352835416793823\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.353213906288147\n",
      "distance to landmark1 is 1.198950171470642\n",
      "distance to landmark2 is 0.5949922204017639\n",
      "distance is 0.8491663932800293\n",
      "distance is 2.260902166366577\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.2078460454940796\n",
      "distance to landmark1 is 1.2622665166854858\n",
      "distance to landmark2 is 2.2835443019866943\n",
      "distance is 1.8140934705734253\n",
      "distance is 2.260902166366577\n",
      "two unit vectors:(array([0.25832713, 0.9660575 ], dtype=float32), array([ 0.820049 , -0.5722933], dtype=float32))\n",
      "dot_product: -0.34102728962898254\n",
      "two unit vectors:(array([0.25832713, 0.9660575 ], dtype=float32), array([ 0.7369867, -0.6759072], dtype=float32))\n",
      "dot_product: -0.4625815153121948\n",
      "two unit vectors:(array([0.25832713, 0.9660575 ], dtype=float32), array([ 0.93049914, -0.36629406], dtype=float32))\n",
      "dot_product: -0.11348795890808105\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.4891717433929443\n",
      "distance to landmark1 is 1.33481764793396\n",
      "distance to landmark2 is 1.39237380027771\n",
      "distance is 0.8751583695411682\n",
      "distance is 1.9241007566452026\n",
      "two unit vectors:(array([-0.5307737, -0.8475137], dtype=float32), array([0.53171164, 0.84692544], dtype=float32))\n",
      "dot_product: -0.9999994039535522\n",
      "two unit vectors:(array([-0.5307737, -0.8475137], dtype=float32), array([0.9749965 , 0.22222018], dtype=float32))\n",
      "dot_product: -0.7058371305465698\n",
      "two unit vectors:(array([-0.5307737, -0.8475137], dtype=float32), array([0.78566915, 0.6186469 ], dtype=float32))\n",
      "dot_product: -0.9413242340087891\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.3470425605773926\n",
      "distance to landmark1 is 1.1922376155853271\n",
      "distance to landmark2 is 0.6029874682426453\n",
      "distance is 0.8751583695411682\n",
      "distance is 2.2818377017974854\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.2191531658172607\n",
      "distance to landmark1 is 1.2791186571121216\n",
      "distance to landmark2 is 2.305739641189575\n",
      "distance is 1.9241007566452026\n",
      "distance is 2.2818377017974854\n",
      "two unit vectors:(array([0.25832713, 0.96605754], dtype=float32), array([ 0.79972035, -0.6003727 ], dtype=float32))\n",
      "dot_product: -0.3734050989151001\n",
      "two unit vectors:(array([0.25832713, 0.96605754], dtype=float32), array([ 0.72460854, -0.68916076], dtype=float32))\n",
      "dot_product: -0.4785829186439514\n",
      "two unit vectors:(array([0.25832713, 0.96605754], dtype=float32), array([ 0.92130756, -0.38883463], dtype=float32))\n",
      "dot_product: -0.13763786852359772\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.5579530000686646\n",
      "distance to landmark1 is 1.403643250465393\n",
      "distance to landmark2 is 1.4417780637741089\n",
      "distance is 0.9020687341690063\n",
      "distance is 2.0088346004486084\n",
      "two unit vectors:(array([-0.9749396 ,  0.22246972], dtype=float32), array([0.5316657, 0.8469543], dtype=float32))\n",
      "dot_product: -0.32992023229599\n",
      "two unit vectors:(array([-0.9749396 ,  0.22246972], dtype=float32), array([0.9669244 , 0.25506303], dtype=float32))\n",
      "dot_product: -0.8859490752220154\n",
      "two unit vectors:(array([-0.9749396 ,  0.22246972], dtype=float32), array([0.7781415 , 0.62808895], dtype=float32))\n",
      "dot_product: -0.6189101934432983\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.2927062511444092\n",
      "distance to landmark1 is 1.137733817100525\n",
      "distance to landmark2 is 0.5813184380531311\n",
      "distance is 0.9020687341690063\n",
      "distance is 2.2551558017730713\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.2287538051605225\n",
      "distance to landmark1 is 1.2927428483963013\n",
      "distance to landmark2 is 2.322875499725342\n",
      "distance is 2.0088346004486084\n",
      "distance is 2.2551558017730713\n",
      "two unit vectors:(array([-0.8597701,  0.5106813], dtype=float32), array([ 0.7842238 , -0.62047815], dtype=float32))\n",
      "dot_product: -0.9911187887191772\n",
      "two unit vectors:(array([-0.8597701,  0.5106813], dtype=float32), array([ 0.7153294, -0.6987874], dtype=float32))\n",
      "dot_product: -0.9718765020370483\n",
      "two unit vectors:(array([-0.8597701,  0.5106813], dtype=float32), array([ 0.91409826, -0.4054928 ], dtype=float32))\n",
      "dot_product: -0.9929919242858887\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.5664879083633423\n",
      "distance to landmark1 is 1.4131639003753662\n",
      "distance to landmark2 is 1.4667333364486694\n",
      "distance is 0.9207897782325745\n",
      "distance is 2.030177354812622\n",
      "two unit vectors:(array([-0.99779856,  0.06631783], dtype=float32), array([0.54747164, 0.8368242 ], dtype=float32))\n",
      "dot_product: -0.4907700717449188\n",
      "two unit vectors:(array([-0.99779856,  0.06631783], dtype=float32), array([0.96915275, 0.24646084], dtype=float32))\n",
      "dot_product: -0.9506744742393494\n",
      "two unit vectors:(array([-0.99779856,  0.06631783], dtype=float32), array([0.7852332, 0.6192001], dtype=float32))\n",
      "dot_product: -0.7424405217170715\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.301774024963379\n",
      "distance to landmark1 is 1.1464351415634155\n",
      "distance to landmark2 is 0.5930396914482117\n",
      "distance is 0.9207897782325745\n",
      "distance is 2.3038456439971924\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.2789332866668701\n",
      "distance to landmark1 is 1.3424980640411377\n",
      "distance to landmark2 is 2.371677875518799\n",
      "distance is 2.030177354812622\n",
      "distance is 2.3038456439971924\n",
      "two unit vectors:(array([-0.85977006,  0.5106812 ], dtype=float32), array([ 0.78729844, -0.61657214], dtype=float32))\n",
      "dot_product: -0.991767406463623\n",
      "two unit vectors:(array([-0.85977006,  0.5106812 ], dtype=float32), array([ 0.7188025, -0.6952143], dtype=float32))\n",
      "dot_product: -0.9730377197265625\n",
      "two unit vectors:(array([-0.85977006,  0.5106812 ], dtype=float32), array([ 0.9128795, -0.408229 ], dtype=float32))\n",
      "dot_product: -0.9933413863182068\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.600066900253296\n",
      "distance to landmark1 is 1.4491734504699707\n",
      "distance to landmark2 is 1.534106731414795\n",
      "distance is 1.0349799394607544\n",
      "distance is 2.039628028869629\n",
      "two unit vectors:(array([-0.99941665,  0.0341523 ], dtype=float32), array([0.5825498, 0.812795 ], dtype=float32))\n",
      "dot_product: -0.5544511675834656\n",
      "two unit vectors:(array([-0.99941665,  0.0341523 ], dtype=float32), array([0.97257715, 0.23258057], dtype=float32))\n",
      "dot_product: -0.9640666246414185\n",
      "two unit vectors:(array([-0.99941665,  0.0341523 ], dtype=float32), array([0.7997183 , 0.60037535], dtype=float32))\n",
      "dot_product: -0.7787476181983948\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.3136265277862549\n",
      "distance to landmark1 is 1.160149335861206\n",
      "distance to landmark2 is 0.5603247880935669\n",
      "distance is 1.0349799394607544\n",
      "distance is 2.3670523166656494\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.3165682554244995\n",
      "distance to landmark1 is 1.379834771156311\n",
      "distance to landmark2 is 2.4083170890808105\n",
      "distance is 2.039628028869629\n",
      "distance is 2.3670523166656494\n",
      "two unit vectors:(array([-0.35257766,  0.9357826 ], dtype=float32), array([ 0.7894473, -0.6138184], dtype=float32))\n",
      "dot_product: -0.852742075920105\n",
      "two unit vectors:(array([-0.35257766,  0.9357826 ], dtype=float32), array([ 0.7213038 , -0.69261867], dtype=float32))\n",
      "dot_product: -0.9024561047554016\n",
      "two unit vectors:(array([-0.35257766,  0.9357826 ], dtype=float32), array([ 0.91200006, -0.41019014], dtype=float32))\n",
      "dot_product: -0.7053996324539185\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.655930995941162\n",
      "distance to landmark1 is 1.5087018013000488\n",
      "distance to landmark2 is 1.633607268333435\n",
      "distance is 1.1207857131958008\n",
      "distance is 2.092379093170166\n",
      "two unit vectors:(array([-0.9994166,  0.0341523], dtype=float32), array([0.62777627, 0.77839375], dtype=float32))\n",
      "dot_product: -0.6008260250091553\n",
      "two unit vectors:(array([-0.9994166,  0.0341523], dtype=float32), array([0.9763354, 0.2162617], dtype=float32))\n",
      "dot_product: -0.9683799743652344\n",
      "two unit vectors:(array([-0.9994166,  0.0341523], dtype=float32), array([0.8180174, 0.5751934], dtype=float32))\n",
      "dot_product: -0.7978959679603577\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.3172069787979126\n",
      "distance to landmark1 is 1.1629574298858643\n",
      "distance to landmark2 is 0.5772742033004761\n",
      "distance is 1.1207857131958008\n",
      "distance is 2.430107831954956\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.3715423345565796\n",
      "distance to landmark1 is 1.4389837980270386\n",
      "distance to landmark2 is 2.470616579055786\n",
      "distance is 2.092379093170166\n",
      "distance is 2.430107831954956\n",
      "two unit vectors:(array([-0.9957272 , -0.09234411], dtype=float32), array([ 0.7738635, -0.6333524], dtype=float32))\n",
      "dot_product: -0.7120705842971802\n",
      "two unit vectors:(array([-0.9957272 , -0.09234411], dtype=float32), array([ 0.7129388, -0.7012263], dtype=float32))\n",
      "dot_product: -0.6451383829116821\n",
      "two unit vectors:(array([-0.9957272 , -0.09234411], dtype=float32), array([ 0.90206134, -0.43160778], dtype=float32))\n",
      "dot_product: -0.8583505749702454\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.7007185220718384\n",
      "distance to landmark1 is 1.5563277006149292\n",
      "distance to landmark2 is 1.7085025310516357\n",
      "distance is 1.1352168321609497\n",
      "distance is 2.0841948986053467\n",
      "two unit vectors:(array([-0.9699621 ,  0.24325629], dtype=float32), array([0.65815884, 0.7528791 ], dtype=float32))\n",
      "dot_product: -0.45524659752845764\n",
      "two unit vectors:(array([-0.9699621 ,  0.24325629], dtype=float32), array([0.9787122 , 0.20523769], dtype=float32))\n",
      "dot_product: -0.8993884325027466\n",
      "two unit vectors:(array([-0.9699621 ,  0.24325629], dtype=float32), array([0.8303502, 0.5572419], dtype=float32))\n",
      "dot_product: -0.6698556542396545\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.3161708116531372\n",
      "distance to landmark1 is 1.1593950986862183\n",
      "distance to landmark2 is 0.631920576095581\n",
      "distance is 1.1352168321609497\n",
      "distance is 2.4089717864990234\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.3858906030654907\n",
      "distance to landmark1 is 1.4520577192306519\n",
      "distance to landmark2 is 2.482449531555176\n",
      "distance is 2.0841948986053467\n",
      "distance is 2.4089717864990234\n",
      "two unit vectors:(array([-0.9998023 , -0.01988686], dtype=float32), array([ 0.7794316, -0.6264873], dtype=float32))\n",
      "dot_product: -0.766818642616272\n",
      "two unit vectors:(array([-0.9998023 , -0.01988686], dtype=float32), array([ 0.71687305, -0.6972038 ], dtype=float32))\n",
      "dot_product: -0.7028661370277405\n",
      "two unit vectors:(array([-0.9998023 , -0.01988686], dtype=float32), array([ 0.9039962 , -0.42754033], dtype=float32))\n",
      "dot_product: -0.8953150510787964\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.7040929794311523\n",
      "distance to landmark1 is 1.5600465536117554\n",
      "distance to landmark2 is 1.715820074081421\n",
      "distance is 1.0460752248764038\n",
      "distance is 2.0845789909362793\n",
      "two unit vectors:(array([-0.96996206,  0.24325627], dtype=float32), array([0.661646  , 0.74981636], dtype=float32))\n",
      "dot_product: -0.45937401056289673\n",
      "two unit vectors:(array([-0.96996206,  0.24325627], dtype=float32), array([0.9791353, 0.2032095], dtype=float32))\n",
      "dot_product: -0.9002920985221863\n",
      "two unit vectors:(array([-0.96996206,  0.24325627], dtype=float32), array([0.83191687, 0.55490017], dtype=float32))\n",
      "dot_product: -0.6719448566436768\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.3171637058258057\n",
      "distance to landmark1 is 1.157273530960083\n",
      "distance to landmark2 is 0.7176029086112976\n",
      "distance is 1.0460752248764038\n",
      "distance is 2.393540143966675\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.4391731023788452\n",
      "distance to landmark1 is 1.50143301486969\n",
      "distance to landmark2 is 2.5276029109954834\n",
      "distance is 2.0845789909362793\n",
      "distance is 2.393540143966675\n",
      "two unit vectors:(array([ 0.92199177, -0.3872095 ], dtype=float32), array([ 0.7961938 , -0.60504174], dtype=float32))\n",
      "dot_product: 0.968362033367157\n",
      "the event: towards\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.7066447734832764\n",
      "distance to landmark1 is 1.5628576278686523\n",
      "distance to landmark2 is 1.7213129997253418\n",
      "distance is 0.980058491230011\n",
      "distance is 2.0815281867980957\n",
      "two unit vectors:(array([-0.09038488, -0.99590695], dtype=float32), array([0.66424114, 0.7475184 ], dtype=float32))\n",
      "dot_product: -0.804496169090271\n",
      "two unit vectors:(array([-0.09038488, -0.99590695], dtype=float32), array([0.9794475 , 0.20169917], dtype=float32))\n",
      "dot_product: -0.2894008457660675\n",
      "two unit vectors:(array([-0.09038488, -0.99590695], dtype=float32), array([0.83308244, 0.55314887], dtype=float32))\n",
      "dot_product: -0.6261828541755676\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.372468113899231\n",
      "distance to landmark1 is 1.2109503746032715\n",
      "distance to landmark2 is 0.8064578175544739\n",
      "distance is 0.980058491230011\n",
      "distance is 2.401705026626587\n",
      "the event: none\n",
      "the RM state: u2\n",
      "the reward: 0\n",
      "distance to landmark0 is 1.4367481470108032\n",
      "distance to landmark1 is 1.499058485031128\n",
      "distance to landmark2 is 2.5253043174743652\n",
      "distance is 2.0815281867980957\n",
      "distance is 2.401705026626587\n",
      "two unit vectors:(array([ 0.9999051 , -0.01377496], dtype=float32), array([ 0.79594666, -0.6053667 ], dtype=float32))\n",
      "dot_product: 0.8042100667953491\n",
      "two unit vectors:(array([ 0.9999051 , -0.01377496], dtype=float32), array([ 0.72901785, -0.68449473], dtype=float32))\n",
      "dot_product: 0.7383775115013123\n",
      "two unit vectors:(array([ 0.9999051 , -0.01377496], dtype=float32), array([ 0.90956897, -0.4155529 ], dtype=float32))\n",
      "dot_product: 0.9152069091796875\n",
      "the event: towards\n",
      "the RM state: u2\n",
      "the reward: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = simple_spread_v3.env()\n",
    "env.reset()\n",
    "\n",
    "# Initialize the Reward Machine\n",
    "reward_machine = RewardMachine()\n",
    "\n",
    "# Run the environment loop\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()    \n",
    "    # Update the Reward Machine\n",
    "    compute_reward(observation, reward_machine)\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # Replace with your policy's action selection\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    # Step the environment\n",
    "    env.step(action)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b17e95-ea6a-4ce7-bb82-0c624483d7cf",
   "metadata": {},
   "source": [
    "# Use stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21665d63-4781-4f2d-8cbf-bcb2fb90ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from pettingzoo.mpe import simple_spread_v2\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "from supersuit import (\n",
    "    pad_action_space_v0,\n",
    "    pad_observations_v0,\n",
    "    pettingzoo_env_to_vec_env_v1,\n",
    "    concat_vec_envs_v1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f2c11c92-e766-4005-a99c-5382f97489f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "num_agents = 3\n",
    "env = simple_spread_v3.parallel_env(\n",
    "    N=num_agents, max_cycles=25, local_ratio=0.5, continuous_actions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a879784-26d9-45c7-8bb5-534098451796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop a custom environment wrapper that incorporates the Reward Machine logic.\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class RewardMachineWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, reward_machines):\n",
    "        super(RewardMachineWrapper, self).__init__(env)\n",
    "        self.reward_machines = reward_machines\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.reward_machine.state = 'u0'\n",
    "        return self.env.reset(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4aafaead-c505-4be5-a43b-06f7c85b487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad action and observation spaces to handle varying spaces among agents\n",
    "env = pad_observations_v0(env)\n",
    "env = pad_action_space_v0(env)\n",
    "# Create Reward Machines for each agent\n",
    "agent_ids = env.possible_agents\n",
    "reward_machines = {agent_id: RewardMachine() for agent_id in agent_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "524a2b85-c330-4709-8ba6-98877c91d016",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "pettingzoo_env_to_vec_env takes in a pettingzoo ParallelEnv. Can create a parallel_env with pistonball.parallel_env() or convert it from an AEC env with `from pettingzoo.utils.conversions import aec_to_parallel; aec_to_parallel(env)``",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m RewardMachineWrapper(env, reward_machines)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert to a vectorized environment\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpettingzoo_env_to_vec_env_v1\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Concatenate vectorized environments for parallel execution\u001b[39;00m\n\u001b[1;32m      8\u001b[0m num_envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# Number of parallel environments\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/learn-stable-baseline3-wCRf4qmO-py3.10/lib/python3.10/site-packages/supersuit/vector/vector_constructors.py:83\u001b[0m, in \u001b[0;36mpettingzoo_env_to_vec_env_v1\u001b[0;34m(parallel_env)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpettingzoo_env_to_vec_env_v1\u001b[39m(parallel_env):\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m     84\u001b[0m         parallel_env, ParallelEnv\n\u001b[1;32m     85\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpettingzoo_env_to_vec_env takes in a pettingzoo ParallelEnv. Can create a parallel_env with pistonball.parallel_env() or convert it from an AEC env with `from pettingzoo.utils.conversions import aec_to_parallel; aec_to_parallel(env)``\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m     87\u001b[0m         parallel_env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpossible_agents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment passed to pettingzoo_env_to_vec_env must have possible_agents attribute.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MarkovVectorEnv(parallel_env)\n",
      "\u001b[0;31mAssertionError\u001b[0m: pettingzoo_env_to_vec_env takes in a pettingzoo ParallelEnv. Can create a parallel_env with pistonball.parallel_env() or convert it from an AEC env with `from pettingzoo.utils.conversions import aec_to_parallel; aec_to_parallel(env)``"
     ]
    }
   ],
   "source": [
    "# Wrap the environment with the RewardMachineWrapper\n",
    "env = RewardMachineWrapper(env, reward_machines)\n",
    "\n",
    "# Convert to a vectorized environment\n",
    "env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "\n",
    "# Concatenate vectorized environments for parallel execution\n",
    "num_envs = 4  # Number of parallel environments\n",
    "env = ss.concat_vec_envs_v1(env, num_envs, num_cpus=1, base_class='stable_baselines3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "478ff373-fc01-4c7f-a2b5-abfcbe18647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert the PettingZoo environment to a vectorized environment\n",
    "env = pettingzoo_env_to_vec_env_v1(env)\n",
    "\n",
    "# Concatenate vectorized environments for parallel execution\n",
    "num_envs = 4  # Number of parallel environments\n",
    "env = concat_vec_envs_v1(env, num_envs, num_cpus=1, base_class='stable_baselines3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4913b18-a1c6-45f3-b296-2b55779383f9",
   "metadata": {},
   "source": [
    "## Vectorized environment's Key Features and Benefits\n",
    "### Parallel Execution:\n",
    "Vectorized environments enable the execution of multiple environments in parallel. This can significantly speed up the data sampling process, especially when using multiple CPU cores or GPUs.\n",
    "### Batched Actions and Observations:\n",
    "Instead of passing a single action and receiving a single observation and reward, vectorized environments handle batches of actions, observations, and rewards. This means that the agent can take multiple actions and receive multiple observations and rewards in each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374238cb-f89e-496c-9053-ee00457f6de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27b4cfca-a29c-4916-b851-1bda1ea445c5",
   "metadata": {},
   "source": [
    "## Train each agent independently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f796a337-5dca-4b1f-9ada-b3694152864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Using cpu device\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the models for each agent (each agent has an independent policy)\n",
    "models = []\n",
    "\n",
    "# Loop over the number of agents to create and configure a PPO model for each\n",
    "for agent_id in range(num_agents):\n",
    "    # Initialize a PPO model with the following parameters:\n",
    "    # - 'MlpPolicy': Indicates the use of a multi-layer perceptron policy network\n",
    "    # - env: The environment in which the agent will be trained\n",
    "    # - verbose=3: Sets the verbosity level to 3 for detailed logging during training\n",
    "    # - device=\"cpu\": Specifies that the model should be trained on the CPU\n",
    "    model = PPO('MlpPolicy', env,\n",
    "                n_steps=1024,  # the number of steps the agent collects in each environment before performing a policy update\n",
    "                verbose=3, device=\"cpu\",\n",
    "               # tensorboard_log=\"./ppo_logs/\"\n",
    "               )\n",
    "    \n",
    "    # Append the initialized model to the models list\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e807e21-6034-4fdd-a46a-e7197d717f71",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'envs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43menvs\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'envs' is not defined"
     ]
    }
   ],
   "source": [
    "len(envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f15ac0-2381-46a8-9ef5-ca86b62242dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'envs' is a list of your base environments for each agent\n",
    "reward_machines = [RewardMachine() for _ in envs]\n",
    "wrapped_envs = [RewardMachineWrapper(env, rm) for env, rm in zip(envs, reward_machines)]\n",
    "models = [PPO('MlpPolicy', env, verbose=1) for env in wrapped_envs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f3a7822-612f-40ad-8f8f-af9018338cd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Training agent 1\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 8293  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 1     |\n",
      "|    total_timesteps | 12288 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 5117        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006126936 |\n",
      "|    clip_fraction        | 0.062       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | -0.00292    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.3        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00433    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 34.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_id, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_id\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_agent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/learn-stable-baseline3-wCRf4qmO-py3.10/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/learn-stable-baseline3-wCRf4qmO-py3.10/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/learn-stable-baseline3-wCRf4qmO-py3.10/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/learn-stable-baseline3-wCRf4qmO-py3.10/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/learn-stable-baseline3-wCRf4qmO-py3.10/lib/python3.10/site-packages/supersuit/vector/sb3_vector_wrapper.py:26\u001b[0m, in \u001b[0;36mSB3VecEnvWrapper.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     observations, rewards, terminations, truncations, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Note: SB3 expects dones to be an np.array\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     dones \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m     29\u001b[0m         [terminations[i] \u001b[38;5;129;01mor\u001b[39;00m truncations[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(terminations))]\n\u001b[1;32m     30\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/learn-stable-baseline3-wCRf4qmO-py3.10/lib/python3.10/site-packages/supersuit/vector/concat_vec_env.py:76\u001b[0m, in \u001b[0;36mConcatVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_saved_actions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/learn-stable-baseline3-wCRf4qmO-py3.10/lib/python3.10/site-packages/supersuit/vector/concat_vec_env.py:84\u001b[0m, in \u001b[0;36mConcatVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     81\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iterate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, actions))\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m venv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvec_envs:\n\u001b[1;32m     83\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 84\u001b[0m         \u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate_actions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_envs\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     90\u001b[0m     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m venv\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m     91\u001b[0m observations, rewards, terminations, truncations, infos \u001b[38;5;241m=\u001b[39m transpose(data)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/learn-stable-baseline3-wCRf4qmO-py3.10/lib/python3.10/site-packages/supersuit/vector/markov_vector_wrapper.py:70\u001b[0m, in \u001b[0;36mMarkovVectorEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     64\u001b[0m agent_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpar_env\u001b[38;5;241m.\u001b[39magents)\n\u001b[1;32m     65\u001b[0m act_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     66\u001b[0m     agent: actions[i]\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpar_env\u001b[38;5;241m.\u001b[39mpossible_agents)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agent_set\n\u001b[1;32m     69\u001b[0m }\n\u001b[0;32m---> 70\u001b[0m observations, rewards, terms, truncs, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpar_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mact_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# adds last observation to info where user can get it\u001b[39;00m\n\u001b[1;32m     73\u001b[0m terminations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfromiter(terms\u001b[38;5;241m.\u001b[39mvalues(), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/learn-stable-baseline3-wCRf4qmO-py3.10/lib/python3.10/site-packages/pettingzoo/utils/conversions.py:207\u001b[0m, in \u001b[0;36maec_to_parallel_wrapper.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    204\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39magent_selection\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Parallel environment wrapper expects agents to step in a cycle.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m         )\n\u001b[1;32m    206\u001b[0m obs, rew, termination, truncation, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39mlast()\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maec_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39magents:\n\u001b[1;32m    209\u001b[0m     rewards[agent] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39mrewards[agent]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/learn-stable-baseline3-wCRf4qmO-py3.10/lib/python3.10/site-packages/supersuit/utils/base_aec_wrapper.py:43\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     42\u001b[0m     agent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magent_selection\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminations\u001b[49m[agent] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncations[agent]):\n\u001b[1;32m     44\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modify_action(agent, action)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/learn-stable-baseline3-wCRf4qmO-py3.10/lib/python3.10/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:79\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.__getattr__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be accessed before reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/learn-stable-baseline3-wCRf4qmO-py3.10/lib/python3.10/site-packages/pettingzoo/utils/wrappers/base.py:25\u001b[0m, in \u001b[0;36mBaseWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cumulative_rewards\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Total number of iterations\n",
    "total_iterations = 100\n",
    "\n",
    "# Number of steps each agent learns per iteration\n",
    "steps_per_agent = 50000\n",
    "\n",
    "# In Stable Baselines3’s implementation of Proximal Policy Optimization (PPO), \n",
    "# the model.learn() function continues training the existing policy rather than initializing a new one with each call.\n",
    "for iteration in range(total_iterations):\n",
    "    for agent_id, model in enumerate(models):\n",
    "        print(f\"Iteration {iteration + 1}, Training agent {agent_id + 1}\")\n",
    "        model.learn(total_timesteps=steps_per_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46645c8a-13b5-488b-8f50-bd880fe74b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Stopping the notebook execution.\")\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87f5a39-8195-44f8-9f4f-ef4d606bb8f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=3, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2fc463-e6a0-452e-b02f-e723d488573c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d07ad3-4d8b-4340-b00d-0aa81ba5cf42",
   "metadata": {},
   "source": [
    "### **How to Interpret Training Output**\n",
    "\n",
    "1. **Monitor Rewards (`ep_rew_mean`):**\n",
    "   - A consistently rising mean reward (`ep_rew_mean`) is a good indicator that the agent is learning to perform better in the environment.\n",
    "   - If the reward plateaus or drops significantly, this could suggest that the agent has reached a suboptimal solution or is struggling to learn.\n",
    "\n",
    "2. **Look at Episode Length (`ep_len_mean`):**\n",
    "   - If your environment terminates episodes upon failure, a longer `ep_len_mean` indicates fewer failures.\n",
    "   - In environments with fixed episode lengths, this value might remain constant.\n",
    "\n",
    "3. **Entropy (`entropy_loss`):**\n",
    "   - Early in training, higher entropy values are expected because the agent is still exploring.\n",
    "   - Over time, entropy should decrease as the agent settles on an optimal policy.\n",
    "\n",
    "4. **Loss Metrics (`value_loss`, `policy_loss`):**\n",
    "   - These should gradually decrease but not hit zero, as they represent the errors the model is correcting.\n",
    "   - Sudden spikes in these values can indicate instability, such as an inappropriate learning rate or poor exploration.\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Issues Observed in Training Output**\n",
    "- **Low FPS:** Indicates inefficiencies in the environment or code.\n",
    "- **Flat Rewards (`ep_rew_mean`):** Indicates that the agent is not learning. Try adjusting hyperparameters like the learning rate, exploration strategy, or reward structure.\n",
    "- **Diverging Loss Values:** A sign of instability. Ensure that your reward function is properly scaled and check for bugs in the environment.\n",
    "\n",
    "---\n",
    "\n",
    "By analysing these outputs, you can fine-tune your training process to achieve better results. Let me know if you want help debugging a specific output!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf71703-d5e4-471b-859a-a0fd37eb6a1a",
   "metadata": {},
   "source": [
    "## **Additional Considerations:**\n",
    "\n",
    "- **Parameter Sharing:** The above setup employs parameter sharing, where a single policy network is shared among all agents. This approach is commonly used in multi-agent reinforcement learning to reduce computational complexity and improve coordination among agents.\n",
    "\n",
    "- **Environment-Specific Adjustments:** Depending on the characteristics of the `simple_spread_v3` environment, you might need to apply additional wrappers or adjustments. For instance, if the environment provides visual observations, you may need to include wrappers for frame stacking or resizing.\n",
    "\n",
    "- **Compatibility Issues:** Be aware of potential compatibility issues between different versions of the libraries. It's advisable to consult the official documentation and release notes for PettingZoo, SuperSuit, and Stable-Baselines3 to ensure seamless integration.\n",
    "\n",
    "By following these steps, you can effectively wrap the `simple_spread_v3` environment using SuperSuit, making it compatible with Stable-Baselines3 for training multi-agent reinforcement learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b88477-0bf7-4751-9f2b-090ec859b242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
